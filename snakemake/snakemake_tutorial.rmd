---
title: "Snakemake tutorial"
author: "Kristen Wells"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    highlight: "tango"
    df_print: "paged"
    self_contained: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  eval = F,
  echo = T
)
```

Snakemake is a "tool to create reproducible and scalable data analysis". Snakemake is written in python and can be easily used on different servers and in different envrionments. I like it because it allows me to be confident that I am doing the exact same analysis on all samples and I can run through all steps of the analysis with one single submission script. It is also very easy to update between projects and you often need to edit just one configuration file.

## Quick tips
* I always do a dry run first. This way if there are any errors you can quickly see those and fix them.
* Use `expand` to analyze multiple samples without needing to type out all the output files individually.
* Use `rule all` as a driver of all output files you want generated.
* Use `params` to pass arguments you don't want to hard code

## Installation

The recommendation is to install snakemake with conda. I've always just directly installed with conda

```{bash}
$ conda install -c bioconda -c conda-forge snakemake
``` 

Personally, I like to put conda into it's own environment so I run

```{bash}
$ conda create -n snakemake
$ conda activate snakemake
$ conda install -c bioconda -c conda-forge snakemake
```

If you do it this way, you will need to activate snakemake before running snakemake or submitting a script that runs snakemake.

```{bash}
$ conda activate snakemake
```

But the website actually recommends an additional step

```{bash}
$ conda install -c conda-forge mamba
$ mamba create -c conda-forge -c bioconda -n snakemake snakemake
```

Like my prefered method, this creates an environment for snakemake and you will need to activate the environment before running anything with snakemake

## Basic usage
To use snakemake you write "rules". These rules are individual steps of your analysis pipeline. For example for my RNA-seq analysis, I have a rule to run fastqc, a rule to make a summary of fastqc output, a rule to run star, a rule to make a summary of star output, a rule to run featureCounts, and a rule to make a count table. But let's start with a simple rule.

All snakemake pipelines need a file called the "Snakefile" *This is not 100% necessary, but if you don't call it Snakefile you must put the name of your snakefile into the command when calling snakemake*. This Snakefile contains all of the code and rules necessary to run your pipeline. There are two options for how to write your rules.

1. You can write all of your rules directly into your Snakefile. This is the way I did it in grad school so you can see this option on my github.
2. You can make scripts for sets of rules (like run fastqc and write a summary) and link to them from the snakefile. I personally like this option because it makes your code much more readable. If someone wants to know how you aligned your reads, they can just look at the alignment file rather than searching through all of your rules. It also makes it easier to share rules between pipelines. For example, you are working with a sample that has both mouse and human sequences so you need to change your alignment step, you can keep the fastqc, adapter trimming, and counting rules and just write a new alignment rule. If you choose this method, you need to link to the files from your Snakefile. 

```{python}
include: "src/rules/rule_set.snake"
```

Snakemake recommends a very specific orginazitional strategy that I like, but you don't need to follow. They recommend the scripts go into a directory named `src` and rules go into a directory named `src/rules`. Analysis scripts can also go into the `src` directory. For reasons we can discuss later I put them here: `src/rules/scripts/`. If you chose to have rule files, they have the `.snake` extension.

### How to write a rule
Let's make a rule that will write the name of a sample to a file. To write the first rule, you need the following structure

```{python}
rule make_file:
    output:
        "results/first_file.txt"
    shell:
        """
        echo "this is a file" > {output}
        """
```

A few things to notice. 

1. You always need to name your rules (and the names need to be unique). 
2. You always need to have an output argument
3. You can run shell commands with the shell argument. If you run a shell command you need the three """ at the start and end of the command
4. You can directly call the output file without needing to type the path again.
5. Any shell command (or set of commands) can work. As long as all of your commands are within the """, you can run as complex of a shell script as you want here.

We can now test what will happen by running this rule using the -np flag
* -n means "dry-run", don't execute
* -p prints out the shell commands that will be excuted

```{bash}
$ snakemake -np results/first_file.txt
```

```
Building DAG of jobs...
Job counts:
    count   jobs
    1   make_file
    1

[Mon Mar 15 14:39:29 2021]
rule make_file:
    output: results/first_file.txt
    jobid: 0


        echo "this is a file" > results/first_file.txt

Job counts:
    count   jobs
    1   make_file
    1
This was a dry-run (flag -n). The order of jobs does not reflect the order of execution.
```

Here we told snakemake what command to run by putting the path to the `output` file, here that was `results/first_file.txt`. Notice that it tells you both the name of the job it will run and the command it will run. At the end it also tells you "job counts" which is an overview of all the jobs that will be run. 

To actually run this, we can run
```{bash}
$ snakemake results/first_file.txt --cores 1
```

```
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
    count   jobs
    1   make_file
    1
Select jobs to execute...

[Mon Mar 15 14:48:16 2021]
rule make_file:
    output: results/first_file.txt
    jobid: 0

[Mon Mar 15 14:48:16 2021]
Finished job 0.
1 of 1 steps (100%) done
Complete log: /beevol/home/wellskri/Analysis/Tutorials/snakemake/.snakemake/log/2021-03-15T144815.335079.snakemake.log
```

This gives you the output of the job. If the job runs successfully, it will tell you the job completed successfully. It also tells you the path to the log file that saved this output. The log files are in hidden directories.

To see the output
```{bash}
$ cd results
$ ls
```

```
first_file.txt
```

```{bash}
$ cd ../
```

One more important point, I didn't have to create a results directory. Snakemake will create output directories for you as long as the directory is part of the path in your `output` argument.

### Generalizing a rule
In addition to writing a rule for one specific file, we can also make rules more general by using something called `wildcards`. One of the most common ways I use wildcards is to make a rule general to any samples, but there are other uses too.

```{python}
rule make_file_general:
    output:
        "results/{sample}_first_file.txt"
    shell:
        """
        echo {wildcards.sample} > {output}
        """
```

Notice that you can use a wildcard in the name of the output file and you can call the wildcard as a variable in the shell command.

We can again run a dry test of this

```{bash}
$ snakemake -np results/sample_1_first_file.txt
```

```
Building DAG of jobs...
Job counts:
    count   jobs
    1   make_file_general
    1

[Mon Mar 15 14:47:04 2021]
rule make_file_general:
    output: results/sample_1_first_file.txt
    jobid: 0
    wildcards: sample=sample_1


        echo sample_1 > results/sample_1_first_file.txt

Job counts:
    count   jobs
    1   make_file_general
    1
This was a dry-run (flag -n). The order of jobs does not reflect the order of execution.
```

Again, it tells us what will be done. Here you can see that the job name is different than the pevious job, that was "make_file" while this is "make_file_general".

Again, we can run it
```{bash}
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
    count   jobs
    1   make_file_general
    1
Select jobs to execute...

[Mon Mar 15 14:52:18 2021]
rule make_file_general:
    output: results/sample_1_first_file.txt
    jobid: 0
    wildcards: sample=sample_1

[Mon Mar 15 14:52:18 2021]
Finished job 0.
1 of 1 steps (100%) done
Complete log: /beevol/home/wellskri/Analysis/Tutorials/snakemake/.snakemake/log/2021-03-15T145218.383705.snakemake.log
```

To see the output
```{bash}
$ cd results
$ ls
```

```
first_file.txt  sample_1_first_file.txt
```

Now we can see both files present.

If the file has already been created, snakemake will not run the rule again

```{bash}
$ snakemake results/sample_1_first_file.txt --cores 1
```

```
Building DAG of jobs...
Nothing to be done.
Complete log: /beevol/home/wellskri/Analysis/Tutorials/snakemake/.snakemake/log/2021-03-15T150541.044925.snakemake.log
```

### Creating multiple rules
One of the powers of snakemake is that multiple rules can be strung together. Here, the output file for one rule becomes the input file for another rule. Before running the second rule, Snakemake will first check that the input file exists. If it doesn't, it will create the input file by running the first rule.

Let's look at an example using our first rule

```{python}
rule make_file_general:
    output:
        "results/{sample}_first_file.txt"
    shell:
        """
        echo {wildcards.sample} > {output}
        """

rule make_second_file:
    input:
        "results/{sample}_first_file.txt"
    output:
        "results/{sample}_second_file.txt"
    shell:
        """
        echo {wildcards.sample} > {output}
        """
```

Here we now have an input file. Just like with the output, we can include wildcards with the input file. Make sure the same wildcard is used in both the input and output files (there are some exceptions that we will get to later).

If snakemake can't locate the input file, it will look to see if any other rules will generate that input file. If it can't find any rules to generate that file, it will throw and error.

```
MissingInputException in line 18 of /beevol/home/wellskri/Analysis/Tutorials/snakemake/Snakefile:
Missing input files for rule make_second_file:
results/sample_1_first_file1.txt
```

If you see an error like this, make sure that you have a rule with an output file that is identical to the input file.

To test this rule we can run
```{bash}
$ snakemake -np results/sample_1_second_file.txt
```

```
Building DAG of jobs...
Job counts:
    count   jobs
    1   make_second_file
    1

[Mon Mar 15 15:06:45 2021]
rule make_second_file:
    input: results/sample_1_first_file.txt
    output: results/sample_1_second_file.txt
    jobid: 0
    wildcards: sample=sample_1


        echo sample_1 > results/sample_1_second_file.txt

Job counts:
    count   jobs
    1   make_second_file
    1
This was a dry-run (flag -n). The order of jobs does not reflect the order of execution.
```

Here you can see that only the second rule will be run. But a great thing about snakemake is it will run everything by checking that the input files exist. To show how this works, I will delete my results folder

```{bash}
$ rm -r results
$ snakemake -np results/sample_1_second_file.txt
```

```
Building DAG of jobs...
Job counts:
    count   jobs
    1   make_file_general
    1   make_second_file
    2

[Mon Mar 15 15:03:38 2021]
rule make_file_general:
    output: results/sample_1_first_file.txt
    jobid: 1
    wildcards: sample=sample_1


        echo sample_1 > results/sample_1_first_file.txt


[Mon Mar 15 15:03:38 2021]
rule make_second_file:
    input: results/sample_1_first_file.txt
    output: results/sample_1_second_file.txt
    jobid: 0
    wildcards: sample=sample_1


        echo sample_1 > results/sample_1_second_file.txt

Job counts:
    count   jobs
    1   make_file_general
    1   make_second_file
    2
This was a dry-run (flag -n). The order of jobs does not reflect the order of execution.
```

Now you can see that even though we only referred to the second file, snakemake will run both rules. 

Again to run it
```{bash}
$ snakemake results/sample_1_second_file.txt --cores 1
```

```
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
    count   jobs
    1   make_second_file
    1
Select jobs to execute...

[Mon Mar 15 15:08:46 2021]
rule make_second_file:
    input: results/sample_1_first_file.txt
    output: results/sample_1_second_file.txt
    jobid: 0
    wildcards: sample=sample_1

[Mon Mar 15 15:08:46 2021]
Finished job 0.
1 of 1 steps (100%) done
Complete log: /beevol/home/wellskri/Analysis/Tutorials/snakemake/.snakemake/log/2021-03-15T150846.110197.snakemake.log
```

We can check the output
```{bash}
$ cd results
$ ls
```

```
sample_1_first_file.txt  sample_1_second_file.txt
```

### Running multiple samples
Snakemake can also help us combine multiple samples and run them at the same time. For example, you can be running an analysis and the last step is to create a combined summary of all the runs. For this, you could create a rule where the input file is the final output from each individual sample. To create this rule you could either list all the expected output samples

```{python}
rule combine_samples:
    input:
        "results/sample_1_second_file.txt",
        "results/sample_2_second_file.txt",
        "results/sample_3_second_file.txt"

```

But this isn't very flexible if you add a new sample or if you want to copy to a new analysis. It also becomes tiring if you want to do this for many samples. Another option is to use `expand`. To use `expand`, first include a list of samples in your snakefile (later we will put these into a `configfile`)

First we add a list to the snakefile
```{python}
samples=["sample_1", "sample_2", "sample_3", "sample_4"]
```

Then we can write our rule
```{python}
rule combine_samples:
    input:
        expand(
            "results/{sample}_second_file.txt",
            sample = samples
        )
    output:
        "results/combined_file.txt"
    shell:
        """
        cat {input} > {output}
        """
```

In this rule we use something called `expand`. This basically will return a list replacing the wildcards with the samples in your list:

```
("results/sample_1_second_file", "results/sample_2_second_file",
"results/sample_3_second_file", "results/sample_4_second_file")
```

If you use two wildcards with expand, it will return all pairwise options:
```{python}
samples = ["sample_1", "sample_2"]
types = ["file", "matrix"]
expand("results/{sample}_second_{type}", sample = samples, type = types)
```

gives

```
("results/sample_1_second_file", "results/sample_1_second_matrix",
"results/sample_2_second_file", "results/sample_2_second_matrix")
```

And do a test run again
```{bash}
$ snakemake -np results/combined_file.txt
```

```
Building DAG of jobs...
Job counts:
    count   jobs
    1   combine_samples
    3   make_file_general
    3   make_second_file
    7

[Mon Mar 15 16:28:12 2021]
rule make_file_general:
    output: results/sample_3_first_file.txt
    jobid: 6
    wildcards: sample=sample_3


        echo sample_3 > results/sample_3_first_file.txt


[Mon Mar 15 16:28:12 2021]
rule make_file_general:
    output: results/sample_4_first_file.txt
    jobid: 8
    wildcards: sample=sample_4


        echo sample_4 > results/sample_4_first_file.txt
...
```


I'm not going to paste the entire output of this command, we can see it when we run it, but a few things I want to draw your attention to. First, look at the job counts table. This shows that 3 separate rules will be run: `combine_samples`, `make_file_general`, and `make_second_file`. Second, look at the count column, this shows that `combine_samples` will be run once, `make_file_general` and `make_second_file` will be run 3 times. This is because we need to run the first two rules for samples 2-4 (we already ran these rules for sample 1). Here you can also see that it is taking the sample names from our list and using them as the sample `wildcards`. 

We can again run this

```{bash}
$ snakemake --cores 1 results/combined_file.txt
$ cd results
$ more combined_file.txt
```

```
sample_1
sample_2
sample_3
sample_4
```

Here we see the output of each individual sample file combined into one file.

### Rule all
Another way to run many samples is by using the `rule all`. `rule all` only takes an input argument and no other arguments.

Here is an example

```{python}
rule four:
    input: "results/{sample}_second_file.txt"
    output: "results/{sample}_fourth_file.txt"
    shell:
        """
        echo {wildcards.sample} > {output}
        """

rule all:
    input:
        expand(
            "results/{sample}_fourth_file.txt",
            sample = samples
        )
```

And again we can run it with a dry run first
```{bash}
$ snakemake -np
```

```
Building DAG of jobs...
Job counts:
    count   jobs
    1   all
    4   four
    5

[Mon Mar 15 17:56:10 2021]
rule four:
    input: results/sample_3_second_file.txt
    output: results/sample_3_fourth_file.txt
    jobid: 7
    wildcards: sample=sample_3


        echo sample_3 > results/sample_3_fourth_file.txt


[Mon Mar 15 17:56:10 2021]
rule four:
    input: results/sample_4_second_file.txt
    output: results/sample_4_fourth_file.txt
    jobid: 10
    wildcards: sample=sample_4


        echo sample_4 > results/sample_4_fourth_file.txt

...
```

There are a couple of things I want to draw attention to here.

1. When you use `rule all` you don't need to specify an output file when calling snakemake. It will look at the output files in `rule all` and generate those
2. One of the jobs listed is `all`. This isn't really going to do anything, but the rule exists to tell snakemake what output files you want to generate.
3. Again here you can see that rule four is being run for all of our samples because we said that we wanted all of our samples run through that rule.
4. We don't generate an extra output file that combines all the outputs from `rule four`, instead we just say we want these files as an output.


`rule all` can be used to walk though your entire pipeline. Let's say here you want `rule four` run on individual samples (but don't plan on combining) and you want `rule combine` run as well. You can add the output from `rule combine` to your `rule all` as well. You can add as many inputs to `rule all` as you want. To add multiple inputs, just separate them by a comma

```{python}
rule all:
    input:
        # Create the fourth file for all samples
        expand(
            "results/{sample}_fourth_file.txt",
            sample = samples
        ),
        # Create the combined file from file two
        "results/combined_file.txt"
```

As you can see, here we are mixing input files with and without `expand`.

I like to put `rule all` in the main snakefile. I also add lots of comments explaining what files are generated. This just helps others work their way through your code. 

We can now run rule all

```{bash}
$ snakemake --cores 1
```

```
...

[Mon Mar 15 18:44:19 2021]
localrule all:
    input: results/sample_1_fourth_file.txt, results/sample_2_fourth_file.txt, results/sample_3_fourth_file.txt, results/sample_4_fourth_file.txt, results/combined_file.txt
    jobid: 0

...
```

You can see that now rule all is generating all the output files we want.

### Adding parameters
In addition to having `input`, `output`, and `shell` in your rules you can add extra parameters with a `params` argument. You can then refer to specific paramaters in the shell command wih {params.param_name}

```{python}
rule params_example:
    input: "results/{sample}_fourth_file.txt"
    output: "results/{sample}_params_ex.txt"
    params:
        file_text = "this is file text",
        other_file = "/beevol/home/wellskri/Analysis/Tutorials/snakemake/results/combined_file.txt"
    shell:
        """
        cp {params.other_file} {output}
        echo {params.file_text} >> {output}
        """
```

Let's add this to rule all and run it

```{python}
rule all:
    input:
        # Create the fourth file for all samples
        expand(
            "results/{sample}_fourth_file.txt",
            sample = samples
        ),
        # Create the combined file from file two
        "results/combined_file.txt",
        # Create a file using params
        expand(
            "results/{sample}_params_ex.txt",
            sample = samples
            )
```

Testing it

```{bash}
$ snakemake -np
```

```
Building DAG of jobs...
Job counts:
    count   jobs
    1   all
    4   params_example
    5

[Mon Mar 15 18:49:05 2021]
rule params_example:
    input: results/sample_3_fourth_file.txt
    output: results/sample_3_params_ex.txt
    jobid: 16
    wildcards: sample=sample_3


        cp /beevol/home/wellskri/Analysis/Tutorials/snakemake/results/combined_file.txt results/sample_3_params_ex.txt
        echo this is file text >> results/sample_3_params_ex.txt
```

Here you can see that it has replaced the {params.name} with the data contained by the parameter data.

Now to run it and check the output
```{bash}
$ snakemake --cores 1
$ cd results
$ more sample_1_params_ex.txt
```

```
sample_1
sample_2
sample_3
sample_4
this is file text
```

While the params argument is a bit silly here, I often use it to pass arguments like the gtf file or other alignment parameters I want more control over and don't want to hard code into the alignment command.

### Adding a log file
We can also add a log file


## The config file

Reading from the config file into the snake file

## More complicated rules
### Python code as the rule

### Running a script as the rule

### Using a function for input (or paramaters)

## Running jobs on the server
Submitting the snakefile

